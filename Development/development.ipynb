{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import TextGen, OpenAI\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n",
    "question = \"Why is the sky blue?\"\n",
    "num_variations = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OpenAi_Chain(question, num_variations):\n",
    "    \n",
    "    llm = OpenAI(openai_api_key = OPENAI_API_KEY,\n",
    "                 temperature = 0.3,\n",
    "                 model_name = \"text-davinci-003\")\n",
    "    \n",
    "    template = \"\"\"\n",
    "    Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "    Instruction: Your task is to generate {num_variations} different unique versions of the given user question. \n",
    "    Generate these versions from different perspectives and in different tones.  \n",
    "    Please take the given question, make these variations and then answer each question \n",
    "    variation in a unique way based off its subtle differences from the others. \n",
    "    please ONLY return the ANSWERS you generate NOT the questions, and make sure you generate ALL {num_variations} anwsers separated by new lines.\n",
    "\n",
    "    Input:{question}\n",
    "\n",
    "    Response:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"question\", \"num_variations\"])\n",
    "\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "    Model_Response = llm_chain({'question': question, 'num_variations': num_variations})\n",
    "    Answer_List = Model_Response[\"text\"]\n",
    "    return Answer_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_llama(question, num_variations):\n",
    "    cmd = [\"python\", \"/home/ghrist/text-generation-webui/server.py\", \"--model\", \"Llama-2-7b-hf\", \"--lora\", \"test-4bit\", \"--api\"]\n",
    "    process = subprocess.Popen(cmd, cwd=\"/home/ghrist/text-generation-webui\")\n",
    "\n",
    "    langchain.debug = True\n",
    "\n",
    "    model_url = \"http://localhost:5000\"\n",
    "\n",
    "    template = \"\"\"\n",
    "    Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "    Instruction: Your task is to generate {num_variations} different unique versions of the given user question. \n",
    "    Generate these versions from different perspectives and in different tones.  \n",
    "    Please take the given question, make these variations and then answer each question \n",
    "    variation in a unique way based off its subtle differences from the others. \n",
    "    please ONLY return the ANSWERS you generate NOT the questions, and make sure you generate ALL {num_variations} anwsers separated by new lines.\n",
    "\n",
    "    Input:{question}\n",
    "\n",
    "    Response:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"question\", \"num_variations\"])\n",
    "    llm = TextGen(model_url=model_url)\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "    Model_Response = llm_chain({'question': question, 'num_variations': num_variations})\n",
    "    Answer_List = Model_Response[\"text\"]\n",
    "    process.terminate()\n",
    "\n",
    "    return Answer_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Answer_Consolidator_Chain(All_Answers):\n",
    "\n",
    "    template = \"\"\"  \n",
    "    I will provide you with a list of answers, all addressing the same question but in slightly varying ways. \n",
    "    Your task is to analyze and consolidate these answers into one cohesive and representative response. \n",
    "    In your consolidation, prioritize the common consensus found in the majority of the answers. \n",
    "    If certain answers deviate or introduce unique points, weigh them less in favor of the prevailing sentiment or information presented by the majority. \n",
    "    Produce a single, ultimate answer that best represents the collective input, controlling for any outliers or randomness.\n",
    "    The list of answers to perform this on are as follows: {All_Answers}\n",
    "    \"\"\" \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"All_Answers\"], \n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    llm = OpenAI(\n",
    "        openai_api_key=OPENAI_API_KEY,\n",
    "        temperature=0.3,\n",
    "        max_tokens=500,\n",
    "        model_name=\"text-davinci-003\"\n",
    "        )\n",
    "\n",
    "    consolidator_chain = LLMChain(llm=llm, prompt=prompt_template, verbose=True)\n",
    "\n",
    "    response = consolidator_chain({All_Answers})\n",
    "    \n",
    "    return response[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########TEST########\n",
    "\n",
    "tv1 = local_llama(question, num_variations)\n",
    "\n",
    "tv2 = OpenAi_Chain(question, num_variations)\n",
    "\n",
    "both_answers = tv1 + \"\\n\" + tv2\n",
    "\n",
    "Answer_Consolidator_Chain(both_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
